{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RNSA Breast Cancer Detector.**\n",
    "\n",
    "## ML:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Librerias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerias\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pydicom\n",
    "from pathlib import Path\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "#Constantes\n",
    "IMGFOLDER_PATH = \"../data/raw/train_images\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cargamos los datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Dataset...\n",
      "==========\n",
      "\n",
      "Dataset dimensions: (54668, 11)\n",
      "\n",
      "Data Preview:\n",
      "\n",
      "   patient_id    image_id   age  cancer  biopsy  invasive  BIRADS  implant  \\\n",
      "0       10006   462822612  61.0       0       0         0     4.0        0   \n",
      "1       10006  1459541791  61.0       0       0         0     4.0        0   \n",
      "2       10006  1864590858  61.0       0       0         0     4.0        0   \n",
      "3       10006  1874946579  61.0       0       0         0     4.0        0   \n",
      "4       10011   220375232  55.0       0       0         0     0.0        0   \n",
      "\n",
      "   density  laterality  view  \n",
      "0      2.0         0.0   0.0  \n",
      "1      2.0         0.0   1.0  \n",
      "2      2.0         1.0   1.0  \n",
      "3      2.0         1.0   0.0  \n",
      "4      1.0         0.0   0.0  \n",
      "\n",
      " · Successfully data readed\n"
     ]
    }
   ],
   "source": [
    "def ReadData():\n",
    "    print(\"Reading Dataset...\\n==========\\n\")\n",
    "\n",
    "    try:\n",
    "        data = pd.read_csv(\"../data/processed/clean_data.csv\")\n",
    "        print(f\"Dataset dimensions: {data.shape}\\n\\nData Preview:\\n\\n{data.head(5)}\\n\\n · Successfully data readed\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"...Failed data readed. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "data = ReadData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Piperline para convertir las imagenes en tensores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetPaths(root, dataset):\n",
    "\n",
    "    #Creamos una lista con el Path de las imagenes.\n",
    "\n",
    "    root = Path(root) # conversion Str -> Path\n",
    "    list_path = list(root.rglob(\"*.dcm\")) # lista de paths\n",
    "\n",
    "    #Mapeamos\n",
    "    map_label = {}\n",
    "\n",
    "    for p in list_path:\n",
    "        patient_id = p.parts[-2] # -> folder del paciente donde están las imagenes.\n",
    "        img_id = p.stem # -> la imagen en cuestion.\n",
    "\n",
    "        try:    \n",
    "            filter = dataset[(dataset[\"patient_id\"].astype(str) == patient_id) & (dataset[\"image_id\"].astype(str) == img_id)] #Importante convertir los valores del dataset en str para la comparacion.\n",
    "\n",
    "            if not filter.empty:\n",
    "                target = int(filter[\"cancer\"].values[0]) # -> tomamos el valor del target.\n",
    "                map_label[p] = target\n",
    "            else:\n",
    "                print(\"No coincidences in dataset.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e} in row {p}.\")\n",
    "\n",
    "    return map_label\n",
    "\n",
    "\n",
    "\n",
    "mappingIMG = GetPaths(IMGFOLDER_PATH, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primero vamos a crear una pequeña funcion para normalizar el tamaño de las imagenes.\n",
    "\n",
    "def NormalizeIMG(img):\n",
    "    \n",
    "    max_val = np.max(img)\n",
    "    img = img.astype(np.float32)\n",
    "\n",
    "    if max_val > 0:\n",
    "        img = img / max_val\n",
    "\n",
    "    #Vamos a hacer una verificación adicional para convertir las imagenes en escala de grises a RGB si fuese necesario.\n",
    "    \n",
    "    if img.ndim == 2: #Escala de grises -> 1 dimension + canal\n",
    "        img = np.stack([img] * 3, axis=-1)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos las imagenes.\n",
    "def ImgLoader(path):\n",
    "\n",
    "    #Convertimos los paths de tf.str a str normal porque si no falla.\n",
    "    try:\n",
    "        str_path = path.numpy().decode(\"utf-8\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "    #Cargamos imgs dicom\n",
    "    dcm = pydicom.dcmread(str_path)\n",
    "    img = dcm.pixel_array.astype(np.float32)\n",
    "\n",
    "    #Normalizamos el tamaño de las imagenes\n",
    "    img = NormalizeIMG(img)\n",
    "    img = tf.image.resize(img, [224,224]) #Redimensionamos a 244x244\n",
    "\n",
    "    #Convertimos la img a tensor\n",
    "\n",
    "    img_tensor = tf.convert_to_tensor(img)\n",
    "\n",
    "    return img_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lo convertimos en tensores\n",
    "def ImgToTensor(path, label):\n",
    "    img = tf.py_function(func=ImgLoader, inp=[path], Tout=tf.float32)\n",
    "    img.set_shape([224, 224, 3]) #Seteamos la imagen a 3 canales\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TensorMaker(datalabels):\n",
    "\n",
    "    #Convertimos el diccionario en listas separando key y value.\n",
    "    paths = list(datalabels.keys())\n",
    "    labels = list(datalabels.values())\n",
    "\n",
    "    #convertimos las listas en tensores.\n",
    "    path_list = []\n",
    "    label_list = []\n",
    "\n",
    "    for p, l in zip(paths, labels):\n",
    "        path_list.append(str(p)) #Path -> str\n",
    "        label_list.append(l)\n",
    "\n",
    "    tensor_paths = tf.constant(path_list)\n",
    "    tensor_labels = tf.constant(label_list, dtype=tf.int32)\n",
    "\n",
    "    #Crearmos el dataframe con los tensores.\n",
    "    data_tensor = tf.data.Dataset.from_tensor_slices((tensor_paths, tensor_labels))\n",
    "\n",
    "    data_tensor = data_tensor.map(ImgToTensor, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    #Aplicamos el batching y el prefetching\n",
    "    data_tensor = data_tensor.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return data_tensor\n",
    "\n",
    "dataTensor = TensorMaker(mappingIMG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (32, 224, 224, 3)\n",
      "Label batch shape: (32,)\n"
     ]
    }
   ],
   "source": [
    "#Vamos a comprobar que se haya creado bien el dataset.\n",
    "\n",
    "for img, label in dataTensor.take(1):\n",
    "    print(f\"Image shape: {img.shape}\")\n",
    "    print(f\"Label batch shape: {label.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Una vez tenemos el Dataset convertido en tensores vamos a separarlo en train y val\n",
    "\n",
    "def SplitData(dataset):\n",
    "    total_size = len(dataset)\n",
    "    val_size = int(total_size * 0.2) #validacion del 20%\n",
    "\n",
    "    data_train = dataset.skip(val_size)\n",
    "    data_val = dataset.take(val_size)\n",
    "\n",
    "    return data_train, data_val\n",
    "\n",
    "dataTensorTrain, dataTensorVal = SplitData(dataTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creando el modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelCreator():\n",
    "    #Cargamos ResNet50V2 preentrenado\n",
    "    prefit_model = ResNet50V2(weights=\"imagenet\", include_top=False, input_shape=(224,224,3))\n",
    "\n",
    "    #Congelamos los pesos del modelo preentrenado\n",
    "    prefit_model.trainable = False\n",
    "\n",
    "    #Añadimos las capas densas adicionales\n",
    "    final_model = models.Sequential([\n",
    "        prefit_model,\n",
    "        layers.GlobalAveragePooling2D(), # -> Reducimos las dimensiones a 2D\n",
    "        layers.Dense(128, activation=\"relu\"), # -> Capa densa\n",
    "        layers.Dropout(0.5), # -> Capa de regularización\n",
    "        layers.Dense(1, activation=\"sigmoid\") #Capa de salida\n",
    "    ])\n",
    "\n",
    "    final_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return final_model\n",
    "\n",
    "model = ModelCreator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrenando al modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23s/step - accuracy: 0.9692 - loss: 0.2301 \n",
      "Epoch 1: val_loss improved from inf to 0.14585, saving model to ../models/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1311s\u001b[0m 28s/step - accuracy: 0.9693 - loss: 0.2294 - val_accuracy: 0.9688 - val_loss: 0.1459\n",
      "Epoch 2/20\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22s/step - accuracy: 0.9828 - loss: 0.0990 \n",
      "Epoch 2: val_loss improved from 0.14585 to 0.12871, saving model to ../models/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1307s\u001b[0m 28s/step - accuracy: 0.9827 - loss: 0.0994 - val_accuracy: 0.9688 - val_loss: 0.1287\n",
      "Epoch 3/20\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31s/step - accuracy: 0.9828 - loss: 0.0801 \n",
      "Epoch 3: val_loss improved from 0.12871 to 0.12331, saving model to ../models/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1780s\u001b[0m 37s/step - accuracy: 0.9827 - loss: 0.0806 - val_accuracy: 0.9688 - val_loss: 0.1233\n",
      "Epoch 4/20\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22s/step - accuracy: 0.9828 - loss: 0.0765 \n",
      "Epoch 4: val_loss did not improve from 0.12331\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1245s\u001b[0m 27s/step - accuracy: 0.9827 - loss: 0.0770 - val_accuracy: 0.9688 - val_loss: 0.1302\n",
      "Epoch 5/20\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21s/step - accuracy: 0.9844 - loss: 0.0724 \n",
      "Epoch 5: val_loss did not improve from 0.12331\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1234s\u001b[0m 27s/step - accuracy: 0.9843 - loss: 0.0727 - val_accuracy: 0.9688 - val_loss: 0.1321\n",
      "Epoch 6/20\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21s/step - accuracy: 0.9824 - loss: 0.0696 \n",
      "Epoch 6: val_loss did not improve from 0.12331\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1234s\u001b[0m 27s/step - accuracy: 0.9823 - loss: 0.0699 - val_accuracy: 0.9688 - val_loss: 0.1386\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n"
     ]
    }
   ],
   "source": [
    "#Definimos los callbacks\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=\"../models/best_model.h5\", monitor=\"val_loss\", save_best_only=True, save_weights_only=False, mode=\"min\", verbose=1)\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, mode=\"min\", verbose=1)\n",
    "\n",
    "#Empezamos el entrenamiento\n",
    "hist = model.fit(dataTensorTrain, validation_data=dataTensorVal, epochs=20, callbacks=[checkpoint, early_stop])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
